{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faa70965-c2d7-469f-8f83-2c7fa5c1969d",
   "metadata": {},
   "source": [
    "# Program - RAG\n",
    "\n",
    "Blake Green"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0dac6a-c82e-424b-bbbf-6e15323f03f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pypdf import PdfReader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "import numpy as np\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Qdrant as QdrantVectorStore\n",
    "from langchain.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "import time\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fe6cf1-bf1d-42ab-9697-1c60b59a2f44",
   "metadata": {},
   "source": [
    "## SECTION 1: Environment Setup & PDF Ingestion\n",
    "\n",
    "- **`load_dotenv()`**  \n",
    "  Loads environment variables from a `.env` file into `os.environ`, so we can keep API keys and paths out of source control.\n",
    "  \n",
    "- **Environment variables**  \n",
    "  ```python\n",
    "  PDF_DIR         = os.getenv(\"PDF_DIR\")\n",
    "  QDRANT_URL      = os.getenv(\"QDRANT_URL\")\n",
    "  QDRANT_API_KEY  = os.getenv(\"QDRANT_API_KEY\")\n",
    "  MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "  ```\n",
    "  Fetches the directory containing PDFs, Qdrant endpoint & key, and the Mistral API key at runtime.\n",
    "\n",
    "### Reading and Limiting PDFs\n",
    "```python\n",
    "all_text = []\n",
    "for fname in os.listdir(PDF_DIR):\n",
    "    if fname.lower().endswith(\".pdf\"):\n",
    "        reader = PdfReader(os.path.join(PDF_DIR, fname))\n",
    "        text = \"\\n\".join(p.extract_text() or \"\" for p in reader.pages)\n",
    "        all_text.append(text)\n",
    "# keep exactly 10 reports (or fewer if there aren't 10)\n",
    "all_text = all_text[:10]\n",
    "```\n",
    "Iterates over every PDF in PDF_DIR, extracts text page-by-page, and collects each document's full text in the all_text list. The code then limits processing to a maximum of 10 documents to manage resource usage.\n",
    "\n",
    "### Optimizing Chunk Size and Overlap\n",
    "```python\n",
    "lengths     = [len(doc) for doc in all_text]\n",
    "avg_length  = sum(lengths) / len(lengths)\n",
    "chunk_size  = int(avg_length * 0.25)\n",
    "chunk_overlap = int(chunk_size * 0.10)\n",
    "print(f\"Using chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")\n",
    "```\n",
    "Calculates optimal chunking parameters based on document statistics:\n",
    "- Measures the length of each document\n",
    "- Computes the average document length\n",
    "- Sets chunk size to 25% of average document length\n",
    "- Sets chunk overlap to 10% of the chunk size\n",
    "- Outputs the calculated parameters for transparency\n",
    "\n",
    "### Document Splitting\n",
    "```python\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len\n",
    ")\n",
    "documents = splitter.create_documents(all_text)\n",
    "```\n",
    "Creates a text splitter that divides documents at paragraph breaks while respecting the calculated chunk parameters, then processes all documents into properly sized chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecad0157-0cb0-4801-b05f-6882d9b9f263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using chunk_size=2914, chunk_overlap=291\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # reads .env into os.environ\n",
    "\n",
    "PDF_DIR         = os.getenv(\"PDF_DIR\")\n",
    "QDRANT_URL      = os.getenv(\"QDRANT_URL\")\n",
    "QDRANT_API_KEY  = os.getenv(\"QDRANT_API_KEY\")\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# ─── 1. Read all PDFs (then grab only 10) ──────────────────────────────────\n",
    "all_text = []\n",
    "for fname in os.listdir(PDF_DIR):\n",
    "    if fname.lower().endswith(\".pdf\"):\n",
    "        reader = PdfReader(os.path.join(PDF_DIR, fname))\n",
    "        text = \"\\n\".join(p.extract_text() or \"\" for p in reader.pages)\n",
    "        all_text.append(text)\n",
    "\n",
    "# keep exactly 10 reports (or fewer if there aren’t 10)\n",
    "all_text = all_text[:10]\n",
    "\n",
    "# ─── 2. Compute “optimal” chunk-size & overlap ─────────────────────────────\n",
    "# e.g. use 25% of the average document length as chunk_size,\n",
    "# and 10% of that as the overlap.\n",
    "lengths     = [len(doc) for doc in all_text]\n",
    "avg_length  = sum(lengths) / len(lengths)\n",
    "chunk_size  = int(avg_length * 0.25)\n",
    "chunk_overlap = int(chunk_size * 0.10)\n",
    "\n",
    "print(f\"Using chunk_size={chunk_size}, chunk_overlap={chunk_overlap}\")\n",
    "\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\\n\",\n",
    "    chunk_size=chunk_size,\n",
    "    chunk_overlap=chunk_overlap,\n",
    "    length_function=len\n",
    ")\n",
    "documents = splitter.create_documents(all_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c76da-ce54-497b-b0b4-28f575907036",
   "metadata": {},
   "source": [
    "## SECTION 2: Embedding Model Definitions\n",
    "\n",
    "```python\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "def embed_sbert(texts):\n",
    "    return sbert_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "instructor_model = SentenceTransformer(\n",
    "    \"hkunlp/instructor-xl\",\n",
    "    trust_remote_code=True,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "def embed_instructor(texts):\n",
    "    return instructor_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "mpnet_model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "def embed_mpnet(texts):\n",
    "    return mpnet_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "```\n",
    "\n",
    "**[all-MiniLM-L6-v2 (SBERT, 384-dim)](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)**  \n",
    "A distilled Sentence-BERT model offering fast, lightweight 384-dim embeddings. Well-suited for large-scale semantic search where inference speed and memory are critical.\n",
    "\n",
    "**[hkunlp/instructor-xl (Instructor-XL, 768-dim)](https://huggingface.co/hkunlp/instructor-xl)**  \n",
    "An instruction-tuned model that conditions its embeddings on supplied prompts, producing 768-dim vectors. Good for tasks where you want to feed explicit instructions or task descriptions into the embedding process.\n",
    "\n",
    "**[all-mpnet-base-v2 (MPNet, 768-dim)](https://huggingface.co/sentence-transformers/all-mpnet-base-v2)**  \n",
    "Based on the MPNet pretraining objective (masked & permuted language modeling), this 768-dim model often outperforms BERT-based alternatives on semantic tasks, striking a balance between quality and compute cost.\n",
    "\n",
    "**Embedding functions**  \n",
    "Each embed_* wrapper converts a list of strings into a NumPy array of dense vectors, ready for upload to a vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46742896-c223-4ba9-ae1b-08c3e67cee8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    }
   ],
   "source": [
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "def embed_sbert(texts):\n",
    "    return sbert_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "instructor_model = SentenceTransformer(\n",
    "    \"hkunlp/instructor-xl\",\n",
    "    trust_remote_code=True,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "def embed_instructor(texts):\n",
    "    return instructor_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "mpnet_model = SentenceTransformer(\n",
    "    \"sentence-transformers/all-mpnet-base-v2\",\n",
    "    device=\"cpu\"\n",
    ")\n",
    "def embed_mpnet(texts):\n",
    "    return mpnet_model.encode(texts, convert_to_numpy=True)\n",
    "\n",
    "# Silence HF symlink warnings (optional)\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab308e7c-2ca0-486e-b00b-f2601e84a48d",
   "metadata": {},
   "source": [
    "## SECTION 3: Qdrant Client Initialization & Collection Setup\n",
    "\n",
    "```python\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    prefer_grpc=False,\n",
    "    api_key=QDRANT_API_KEY\n",
    ")\n",
    "\n",
    "collections = {\"sbert\": 384, \"instructor\": 768, \"mpnet\": 768}\n",
    "for name, dim in collections.items():\n",
    "    client.recreate_collection(\n",
    "        collection_name=f\"{name}_collection\",\n",
    "        vectors_config=VectorParams(size=dim, distance=Distance.COSINE)\n",
    "    )\n",
    "```\n",
    "\n",
    "**QdrantClient**  \n",
    "Connects to your Qdrant vector database using HTTP (or gRPC if prefer_grpc=True) and your API key.\n",
    "\n",
    "**recreate_collection**  \n",
    "For each embedding type (sbert, instructor, mpnet), drops any existing collection of that name and creates a new one with the appropriate vector dimension and cosine similarity as the distance metric.\n",
    "\n",
    "## SECTION 4: Bulk Uploading Embeddings to Qdrant\n",
    "\n",
    "```python\n",
    "for name, embed_fn in [(\"sbert\", embed_sbert),\n",
    "                       (\"instructor\", embed_instructor),\n",
    "                       (\"mpnet\", embed_mpnet)]:\n",
    "    texts      = [doc.page_content for doc in documents]\n",
    "    embeddings = embed_fn(texts)\n",
    "    points     = [\n",
    "        PointStruct(id=i, vector=embeddings[i], payload={\"text\": texts[i]})\n",
    "        for i in range(len(texts))\n",
    "    ]\n",
    "    client.upload_points(\n",
    "        collection_name=f\"{name}_collection\",\n",
    "        points=points\n",
    "    )\n",
    "```\n",
    "\n",
    "**Text extraction**  \n",
    "Gathers the .page_content from each chunked Document.\n",
    "\n",
    "**Embedding generation**  \n",
    "Calls the appropriate embed_fn to get a NumPy array of shape (num_chunks, dim).\n",
    "\n",
    "**Point creation & upload**  \n",
    "Wraps each vector in a PointStruct, attaching the original text as payload, and uploads them in bulk to Qdrant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179d9048-b8a2-43a9-870c-32d2da17455f",
   "metadata": {},
   "source": [
    "https://cloud.qdrant.io/accounts/540c85ca-df0c-42c4-adf7-a1747bc5b781/clusters/9a2caa1e-4336-4898-b431-a9d4ed717c4b/api-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82e28c78-338a-47e1-8e36-fc3a284510d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\n",
    "    url=QDRANT_URL,\n",
    "    prefer_grpc=False,\n",
    "    api_key=QDRANT_API_KEY\n",
    ")\n",
    "\n",
    "# Create one collection per embedding\n",
    "collections = {\"sbert\": 384, \"instructor\": 768, \"mpnet\": 768}\n",
    "for name, dim in collections.items():\n",
    "    client.recreate_collection(\n",
    "        collection_name=f\"{name}_collection\",\n",
    "        vectors_config=VectorParams(size=dim, distance=Distance.COSINE)\n",
    "    )\n",
    "\n",
    "# Upload embeddings\n",
    "for name, embed_fn in [(\"sbert\", embed_sbert),\n",
    "                       (\"instructor\", embed_instructor),\n",
    "                       (\"mpnet\", embed_mpnet)]:\n",
    "    texts     = [doc.page_content for doc in documents]\n",
    "    embeddings= embed_fn(texts)\n",
    "    points    = [\n",
    "        PointStruct(id=i, vector=embeddings[i], payload={\"text\": texts[i]})\n",
    "        for i in range(len(texts))\n",
    "    ]\n",
    "    client.upload_points(collection_name=f\"{name}_collection\",\n",
    "                         points=points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de76849d-ae04-40b0-9ce5-775a917d9bfc",
   "metadata": {},
   "source": [
    "## SECTION 5: LLM Setup & Question Bank Definition\n",
    "\n",
    "```python\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small\",\n",
    "    api_key=MISTRAL_API_KEY,\n",
    "    temperature=0.0,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "rag_configs = {\n",
    "    \"sbert_collection\":      \"all-MiniLM-L6-v2\",\n",
    "    \"instructor_collection\": \"hkunlp/instructor-xl\",\n",
    "    \"mpnet_collection\":      \"sentence-transformers/all-mpnet-base-v2\",\n",
    "}\n",
    "\n",
    "multi_questions = [\n",
    "    \"What accident was the most dangerous?\",\n",
    "    \"What accident featured the least amount of people onboard?\",\n",
    "    \"How many accidents featured serious, but non-fatal injuries?\",\n",
    "    \"What accident featured the oldest pilot?\",\n",
    "]\n",
    "\n",
    "single_questions = [\n",
    "    \"What was the cause of incident DCA22LA182?\",\n",
    "    \"When was the captain of incident DCA23LA384 hired, and by which airline?\",\n",
    "    \"In the fatal aircraft incident, was the name of the airshow it occurred at?\",\n",
    "    \"What object was struck in accident DCA21LA137, and which wing sustained damage?\",\n",
    "]\n",
    "```\n",
    "\n",
    "**ChatMistralAI**  \n",
    "Instantiates a zero-temperature Mistral-small chat model for deterministic outputs, with retry logic on transient failures.\n",
    "\n",
    "**rag_configs**  \n",
    "Maps each Qdrant collection name to its original Hugging Face model identifier, used by SentenceTransformerEmbeddings.\n",
    "\n",
    "**Question lists**\n",
    "- **multi_questions**: Broad, comparative queries over the entire corpus.\n",
    "- **single_questions**: Specific, incident-level inquiries keyed to particular report IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e71a38b7-9139-4e92-98dc-21ecc5af4d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small\",\n",
    "    api_key=MISTRAL_API_KEY,\n",
    "    temperature=0.0,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "rag_configs = {\n",
    "    \"sbert_collection\":      \"all-MiniLM-L6-v2\",\n",
    "    \"instructor_collection\": \"hkunlp/instructor-xl\",\n",
    "    \"mpnet_collection\":      \"sentence-transformers/all-mpnet-base-v2\",\n",
    "}\n",
    "\n",
    "multi_questions = [\n",
    "    \"What accident was the most dangerous?\",\n",
    "    \"What accident featured the least amount of people onboard?\",\n",
    "    \"How many accidents featured serious, but non-fatal injuries?\",\n",
    "    \"What accident featured the oldest pilot?\",\n",
    "]\n",
    "\n",
    "single_questions = [\n",
    "    \"What was the cause of incident DCA22LA182?\",\n",
    "    \"When was the captain of incident DCA23LA384 hired, and by which airline?\",\n",
    "    \"In the fatal aircraft incident, was the name of the airshow it occurred at?\",\n",
    "    \"What object was struck in accident DCA21LA137, and which wing sustained damage?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1608651d-de93-42fb-8de6-f5714fffe10e",
   "metadata": {},
   "source": [
    "## SECTION 6: Executing MultiRAG Queries\n",
    "\n",
    "```python\n",
    "for coll_name, emb_model in rag_configs.items():\n",
    "    print(f\"\\n=== MultiRAG using {coll_name} ({emb_model}) ===\")\n",
    "    embeddings  = SentenceTransformerEmbeddings(model_name=emb_model)\n",
    "    vectorstore = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=coll_name,\n",
    "        embeddings=embeddings,\n",
    "        content_payload_key=\"text\",\n",
    "    )\n",
    "    qa_multi = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "    )\n",
    "    for q in multi_questions:\n",
    "        print(f\"\\nQ: {q}\")\n",
    "        ans = qa_multi.run(q)\n",
    "        print(f\"A: {ans}\")\n",
    "        time.sleep(30)\n",
    "```\n",
    "\n",
    "**Building each retriever**  \n",
    "Wraps the Qdrant collection in a LangChain QdrantVectorStore, telling it how to embed new queries and which payload field holds the text.\n",
    "\n",
    "**RetrievalQA.from_chain_type(\"stuff\")**  \n",
    "Creates a simple \"stuff\" chain that concatenates the top-k retrieved chunks (here, k=10) and feeds them to the LLM.\n",
    "\n",
    "**Query loop**  \n",
    "For each of the four multi-document questions, prints the question, retrieves & answers it, then waits 30 seconds to avoid rate limits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716e50fb-abe5-488d-884f-bafa101aca5f",
   "metadata": {},
   "source": [
    "https://admiralcloudberg.medium.com/passing-the-buck-the-story-of-the-2022-wings-over-dallas-air-show-collision-9bbe5947297b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4e4082-bcc1-405b-a1d4-85ea0f8aac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MultiRAG using sbert_collection (all-MiniLM-L6-v2) ===\n",
      "\n",
      "Q: What accident was the most dangerous?\n",
      "A: Based on the information provided, the most dangerous accident in terms of the number of fatalities was the midair collision between a Boeing B17 and a Bell P63 during an air show at Dallas Executive Airport on November 12, 2022, which resulted in 6 fatalities (5 on the Boeing B17 and 1 on the Bell P63). However, it is important to note that the severity of an accident can also be measured in other ways, such as the extent of damage to the aircraft or the impact on the environment.\n",
      "\n",
      "Q: What accident featured the least amount of people onboard?\n",
      "A: The accident with the least amount of people onboard was CEN24LA035, which involved a single-engine airplane with two people onboard.\n",
      "\n",
      "Q: How many accidents featured serious, but non-fatal injuries?\n",
      "A: In the provided reports, there are two accidents (DCA21LA058 and DCA23LA304) where a flight attendant sustained serious injuries but there were no fatalities among the passengers or crew members.\n",
      "\n",
      "Q: What accident featured the oldest pilot?\n",
      "A: The oldest pilot was the captain of the accident featured in report DCA21LA137, who was 62 years old at the time of the accident.\n",
      "\n",
      "=== MultiRAG using instructor_collection (hkunlp/instructor-xl) ===\n",
      "\n",
      "Q: What accident was the most dangerous?\n",
      "A: Based on the information provided, the most dangerous accident in terms of loss of life and damage to the aircraft was the mid-air collision between a Boeing B17 and a Bell P63 during an air show in Dallas, Texas on November 12, 2022. This accident resulted in the death of 6 people and the destruction of both aircraft. However, it is important to note that the severity of an accident can also be measured in terms of its impact on the aviation industry, the environment, and public confidence in air travel.\n",
      "\n",
      "Q: What accident featured the least amount of people onboard?\n",
      "A: The accident with the least amount of people onboard was CEN23MA034, which had 10 people onboard (5 fatalities).\n",
      "\n",
      "Q: How many accidents featured serious, but non-fatal injuries?\n",
      "A: In the first accident you provided, one flight attendant sustained serious injuries. In the second accident, one flight attendant was diagnosed with a fractured pelvis. In the third accident, one flight attendant was diagnosed with a spinal compression fracture. Therefore, a total of 3 accidents featured serious, but non-fatal injuries.\n",
      "\n",
      "Q: What accident featured the oldest pilot?\n",
      "A: The oldest pilot mentioned in the provided reports is the captain of the SkyWest flight 5069, who was 58 years old at the time of the incident.\n",
      "\n",
      "=== MultiRAG using mpnet_collection (sentence-transformers/all-mpnet-base-v2) ===\n",
      "\n",
      "Q: What accident was the most dangerous?\n",
      "A: Based on the information provided, the most dangerous accident in terms of injuries and aircraft damage appears to be the birdstrike accident (CEN24LA035) involving a Grassman Timothy B RV-14A aircraft. Although there were no injuries to the occupants, the airplane sustained substantial damage to the vertical stabilizer due to the birdstrike. However, if we consider the potential for a more severe outcome, the accident involving United Airlines flight 1288 (DCA23LA304) could also be considered dangerous as it resulted in one serious injury to a flight attendant due to convective turbulence.\n",
      "\n",
      "Q: What accident featured the least amount of people onboard?\n",
      "A: The accident that featured the least amount of people onboard is CEN24LA035, which involved a GRASSMAN TIMOTHY B RV-14A with 2 seats and 1 person onboard.\n",
      "\n",
      "Q: How many accidents featured serious, but non-fatal injuries?\n",
      "A: Based on the information provided, there were three accidents (DCA23LA196, DCA23LA304, and DCA21LA058) that featured serious, but non-fatal injuries.\n",
      "\n",
      "Q: What accident featured the oldest pilot?\n",
      "A: The oldest pilot mentioned in the provided reports is the captain of flight DCA21LA058, who was 60 years old at the time of the accident.\n"
     ]
    }
   ],
   "source": [
    "for coll_name, emb_model in rag_configs.items():\n",
    "    print(f\"\\n=== MultiRAG using {coll_name} ({emb_model}) ===\")\n",
    "    embeddings  = SentenceTransformerEmbeddings(model_name=emb_model)\n",
    "    vectorstore = QdrantVectorStore(\n",
    "        client=client,\n",
    "        collection_name=coll_name,\n",
    "        embeddings=embeddings,\n",
    "        content_payload_key=\"text\",\n",
    "    )\n",
    "    qa_multi = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "    )\n",
    "    for q in multi_questions:\n",
    "        print(f\"\\nQ: {q}\")\n",
    "        ans = qa_multi.run(q)\n",
    "        print(f\"A: {ans}\")\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcaf203-a982-4038-b1a6-1af1feb77d89",
   "metadata": {},
   "source": [
    "# MultiRAG Response Analysis\n",
    "\n",
    "## Multiple-Document Questions\n",
    "\n",
    "### 1. What accident was the most dangerous?\n",
    "* **Expected:** CEN23MA034, a midair collision at an airshow that resulted in 6 fatalities.\n",
    "* **SBERT** (`all-MiniLM-L6-v2`):\n",
    "   * ✅ Correctly identifies the midair collision between a B-17 and P-63 with 6 fatalities.\n",
    "   * ❌ Omits the report code (CEN23MA034).\n",
    "* **Instructor** (`hkunlp/instructor-xl`):\n",
    "   * ✅ Identifies the same collision with 6 fatalities.\n",
    "   * ❌ Also omits the specific report ID.\n",
    "* **MPNet** (`all-mpnet-base-v2`):\n",
    "   * ❌ Misattributes \"most dangerous\" to a birdstrike (CEN24LA035), citing aircraft damage rather than fatalities.\n",
    "\n",
    "### 2. What accident featured the least amount of people onboard?\n",
    "* **Expected:** CEN24LA035, with 1 person onboard.\n",
    "* **SBERT:**\n",
    "   * ✅ Correct report (CEN24LA035).\n",
    "   * ❌ Incorrect count (\"two people onboard\" vs. 1).\n",
    "* **Instructor:**\n",
    "   * ❌ Picks CEN23MA034 (10 onboard), entirely off.\n",
    "* **MPNet:**\n",
    "   * ✅ Correctly reports CEN24LA035 with 1 person onboard.\n",
    "\n",
    "### 3. How many accidents featured serious, but non-fatal injuries?\n",
    "* **Expected:** Three (DCA21LA058, DCA23LA196, DCA23LA304).\n",
    "* **SBERT:**\n",
    "   * ❌ Under-counts (only 2 accidents).\n",
    "* **Instructor:**\n",
    "   * ✅ Counts three, correctly lists three distinct serious-injury cases.\n",
    "* **MPNet:**\n",
    "   * ✅ Also counts three matching the expected set.\n",
    "\n",
    "### 4. What accident featured the oldest pilot?\n",
    "* **Expected:** CEN23MA034 (pilot aged 67).\n",
    "* **SBERT:**\n",
    "   * ❌ Attributes it to DCA21LA137 (62 years old).\n",
    "* **Instructor:**\n",
    "   * ❌ Attributes it to SkyWest 5069 (58 years old).\n",
    "* **MPNet:**\n",
    "   * ❌ Attributes it to DCA21LA058 (60 years old).\n",
    "\n",
    "## Overall MultiRAG Comparison\n",
    "* **Best model:** Instructor-XL and MPNet tie on question 3 but only **SBERT** and **Instructor-XL** correctly handle question 1.\n",
    "* **Major strength of Instructor-XL:** Precise counts on injuries (Q 3).\n",
    "* **Major weakness of MPNet:** Misinterprets \"most dangerous\" (Q 1).\n",
    "* **Common gap:** None of the three models correctly identified the oldest pilot (Q 4) or cited report codes consistently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bbde4e-3487-4e91-91e5-3f722be9267e",
   "metadata": {},
   "source": [
    "## SECTION 7: Executing SingleRAG Queries\n",
    "\n",
    "```python\n",
    "for coll_name, emb_model in rag_configs.items():\n",
    "    print(f\"\\n=== SingleRAG using {coll_name} ({emb_model}) ===\")\n",
    "    qa_single = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "    )\n",
    "    for q in single_questions:\n",
    "        print(f\"\\nQ: {q}\")\n",
    "        ans = qa_single.run(q)\n",
    "        print(f\"A: {ans}\")\n",
    "        time.sleep(30)\n",
    "```\n",
    "\n",
    "Reuses the same retriever setup for each embedding collection.\n",
    "\n",
    "Single-incident queries ask about one report at a time; the loop structure is identical to Section 6 but iterates over the single_questions list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b12a3ab-669c-4ef4-8185-460d16352c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SingleRAG using sbert_collection (all-MiniLM-L6-v2) ===\n",
      "\n",
      "Q: What was the cause of incident DCA22LA182?\n",
      "A: The National Transportation Safety Board (NTSB) determined the probable cause of incident DCA22LA182 to be a lateral runway excursion during landing for reasons that could not be determined based on the available evidence.\n",
      "\n",
      "Q: When was the captain of incident DCA23LA384 hired, and by which airline?\n",
      "A: The captain of incident DCA23LA384 was hired on June 10, 2010, by United Airlines.\n",
      "\n",
      "Q: In the fatal aircraft incident, was the name of the airshow it occurred at?\n",
      "A: The name of the airshow where the fatal aircraft incident occurred is the Commemorative Air Force’s (CAF) Wings Over Dallas air show.\n",
      "\n",
      "Q: What object was struck in accident DCA21LA137, and which wing sustained damage?\n",
      "A: In accident DCA21LA137, the left wing of the airplane struck a light pole.\n",
      "\n",
      "=== SingleRAG using instructor_collection (hkunlp/instructor-xl) ===\n",
      "\n",
      "Q: What was the cause of incident DCA22LA182?\n",
      "A: The National Transportation Safety Board (NTSB) determined the probable cause of incident DCA22LA182 to be a lateral runway excursion during landing for reasons that could not be determined based on the available evidence.\n",
      "\n",
      "Q: When was the captain of incident DCA23LA384 hired, and by which airline?\n",
      "A: The captain of incident DCA23LA384 was hired on June 10, 2020, by United Airlines.\n",
      "\n",
      "Q: In the fatal aircraft incident, was the name of the airshow it occurred at?\n",
      "A: The name of the airshow where the fatal aircraft incident occurred was the Commemorative Air Force’s (CAF) Wings Over Dallas air show at Dallas Executive Airport (KRBD) in Dallas, Texas.\n",
      "\n",
      "Q: What object was struck in accident DCA21LA137, and which wing sustained damage?\n",
      "A: In accident DCA21LA137, the left wing of the airplane struck a light pole.\n",
      "\n",
      "=== SingleRAG using mpnet_collection (sentence-transformers/all-mpnet-base-v2) ===\n",
      "\n",
      "Q: What was the cause of incident DCA22LA182?\n",
      "A: The National Transportation Safety Board (NTSB) determined the probable cause of incident DCA22LA182 to be a lateral runway excursion during landing for reasons that could not be determined based on the available evidence.\n",
      "\n",
      "Q: When was the captain of incident DCA23LA384 hired, and by which airline?\n",
      "A: The captain of incident DCA23LA384 was hired on June 10, 2020, by United Airlines.\n",
      "\n",
      "Q: In the fatal aircraft incident, was the name of the airshow it occurred at?\n",
      "A: The name of the airshow where the fatal aircraft incident occurred was the Commemorative Air Force’s (CAF) Wings Over Dallas air show at Dallas Executive Airport (KRBD) in Dallas, Texas.\n",
      "\n",
      "Q: What object was struck in accident DCA21LA137, and which wing sustained damage?\n",
      "A: In accident DCA21LA137, the left wing of the airplane struck a light pole.\n"
     ]
    }
   ],
   "source": [
    "for coll_name, emb_model in rag_configs.items():\n",
    "    print(f\"\\n=== SingleRAG using {coll_name} ({emb_model}) ===\")\n",
    "    qa_single = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 8})\n",
    "    )\n",
    "    for q in single_questions:\n",
    "        print(f\"\\nQ: {q}\")\n",
    "        ans = qa_single.run(q)\n",
    "        print(f\"A: {ans}\")\n",
    "        time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1d52a9-918e-4945-82e4-37eeedc5f061",
   "metadata": {},
   "source": [
    "## SingleRAG Response Analysis\n",
    "\n",
    "### 1. What was the cause of incident DCA22LA182?\n",
    "* **Expected:** A lateral runway excursion during landing (undetermined reason).\n",
    "* **All models (SBERT, Instructor, MPNet):**\n",
    "   * ✅ Perfect match on wording and substance.\n",
    "\n",
    "### 2. When was the captain of incident DCA23LA384 hired, and by which airline?\n",
    "* **Expected:** Hired by United Airlines in April 2001 as a first officer.\n",
    "* **SBERT:**\n",
    "   * ❌ \"June 10, 2010\" by United.\n",
    "* **Instructor:**\n",
    "   * ❌ \"June 10, 2020\" by United.\n",
    "* **MPNet:**\n",
    "   * ❌ Same \"June 10, 2020.\"\n",
    "* **Analysis:** All three hallucinate both date and rank nuance; none recall April 2001.\n",
    "\n",
    "### 3. In the fatal aircraft incident, was the name of the airshow it occurred at?\n",
    "* **Expected:** Commemorative Air Force's Wings Over Dallas.\n",
    "* **SBERT:**\n",
    "   * ✅ Correct name.\n",
    "* **Instructor & MPNet:**\n",
    "   * ✅ Also mention \"CAF Wings Over Dallas\" (plus location detail in Instructor).\n",
    "\n",
    "### 4. What object was struck in accident DCA21LA137, and which wing sustained damage?\n",
    "* **Expected:** Light pole struck left wing.\n",
    "* **All models:**\n",
    "   * ✅ Correctly cite \"left wing\" and \"light pole.\"\n",
    "\n",
    "## Overall SingleRAG Comparison\n",
    "* **Strengths:** All embeddings reliably retrieve cause (Q 1), airshow name (Q 3), and struck object with wing (Q 4).\n",
    "* **Weaknesses:** All three fail the hiring date question (Q 2), suggesting that employment-history details may be sparsely represented or harder to retrieve.\n",
    "* **Best model:** All perform similarly; SBERT has a slight edge on date precision for Q 2 (2010 vs. 2020), but still incorrect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b89df27-6a1b-44c0-a5b4-84e4265492b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
